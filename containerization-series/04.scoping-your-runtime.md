# Containerizing IBM ACE: A Blog Series - Scoping your runtimes

Packing containers isn’t complicated, right? Just throw everything in there. Well, it is. Until it isn’t. Stack too much 
in one and you overload. Mix fragile with flammable and things get risky. Leave half of it empty and you're wasting capacity. Sound familiar?

Let’s be real. “One app per container” sounds clean on paper, but you’d be staring at hundreds of containers before lunch.
So you bundle. But how?

The trick is knowing what belongs together and what absolutely doesn't. This post lays out the logic. Grounded, brutal, 
and battle-tested. How to scope your runtimes without blowing up performance, security, or your own sanity.


## Performance and startup

Some loads are just heavy. You don't toss an engine block into a shipping container without expecting extra handling, 
extra time, and a few complaints from whoever has to lift it. Same goes for big schemas. They blow up compile times and 
drag out cold starts.
That first sentence probably deserves a _that’s what she said_, but we’re being professional. Mostly.

So what do you do? You front-load the work. Compile at build time, not runtime. Get those hits out of the way before 
anything lands in prod. Track cold start behavior in test or UAT. Know what it costs to spin something up. Set a 
threshold that makes sense for your environment, and alert when you blow past it.
Curious how to do that? I blogged about that: [Bar Build Commands: Unraveling the Differences](https://community.ibm.com/community/user/blogs/matthias-blomme/2023/05/23/ace-bar-build-commands-unraveling-the-differences)

If you don’t measure startup time, you’re just hoping it’s fast enough. Spoiler: it won’t be.


## Resource interference

You wouldn’t pack volatile chemicals next to fresh produce. If one leaks, everything’s toast. Same deal with runtimes. 
Put two unrelated applications in the same container and one bad actor can tank the whole thing.

A CPU spike in one flow means the other gets starved. A memory leak in some forgotten listener takes down your critical 
path. Or worse, one flow mounts a file system the other has no business even seeing. Now you’ve got an attack surface no 
one asked for.

If you can't explain, clearly and in one sentence, why two peas are in the same pod, they shouldn't be.
“Because we always did it that way” is **NOT** a reason. It's a red flag. If that’s your answer, stop what you’re doing and 
go fix it. And don’t say it out loud. Ever.

When in doubt, split. Isolation costs less than cleanup.


## 3. Bundling logic

Bundling isn’t inherently bad. But bundling for the wrong reasons will rot your runtime from the inside out.
The points below are just that: points. Not rules. Not guarantees. They’re starting positions. Use your brain.

Good reasons to bundle:
- They hit the same endpoint or interface and have the same criticality.
- They’re owned and operated by the same team. One team, one runtime is easier to live with.
- They’re a small set of resources that share a library and get updated together.
- They’re part of a bigger, tightly coupled setup that moves as one.
- They’re legacy apps with low change rates or a clear decommission plan. Keeping them together makes cleanup easy. 
No one wants to babysit a runtime for something that dies next quarter.

Bad reasons to bundle:
- You want to scale them differently.
- One is critical, the other isn’t. Don’t mix a payment flow with a reporting job and hope for the best.
- They handle data with different sensitivity levels.
- They’re loosely coupled and can fail independently. That’s a feature, not a flaw.
- You’re ignoring technical spillover. In ACE, if one app needs a feature, ibmint optimize keeps it on for all. One weird 
edge case, and nine other apps pay for it.
- You’re bundling out of convenience. Just because two flows came from the same BAR file doesn’t mean they belong together. 
That’s packaging, not architecture.
- You’re crossing team boundaries. If the apps don’t share ownership, support, or deployment cadence, you’re building in 
friction. Someone always ends up blocked.

If you can’t defend the bundle under fire, it’s not a bundle. It’s baggage.

## 4. Libraries and shared code

Reusing code is fine, until it isn't. 



If several tiny resources reuse the same library and you always upgrade them together, they can live together.

If a library update should not affect all resources at once, separate them to reduce the blast radius.

Dare to split up your libraries. If 50% of your flows use library subflow A and 50% user library subflow 5, convert this into
2 seperate libs

## 5. Scaling strategy

If something must scale out, do not package it with something that does not.

Prefer one scaling unit per runtime. It keeps autoscaling rules simple and predictable.

## 6. Security and data sensitivity

Group resources with similar sensitivity and trust level.

Do not bundle a sensitive flow with a less secure one if that weakens the overall posture or complicates audits.

## 7. Ownership and operations

Group by support boundary. If the on call or deployment cadence is different, split.

Align runtime boundaries with who deploys, who monitors, and who fixes.

## 8. Coupling level

End to end chains that are tightly coupled and must move together can be grouped.

Loosely coupled pieces should not share a runtime. Independent failure is a feature, not a bug.

## 9. Build and release shape

Compile at build time. Keep runtime work minimal.

If you use fry, verify that the artifact that passed tests is the one you deploy.

If you use bake, you are wrong.

## 10. A few more worth adding

Licensing cost: more CPU usually means more PVU and more money. Include licensing in your scaling plan.

Observability and blast radius: if a runtime fails, which services and dashboards are down. Keep failure domains small.

Configuration variance: if two apps need very different config or secrets, it is a hint to split.

Upgrade cadence: if one piece changes weekly and another quarterly, consider separate runtimes.